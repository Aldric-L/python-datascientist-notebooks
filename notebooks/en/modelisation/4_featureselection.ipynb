{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3efbf0e-f33d-4ed5-b1e5-36bf63a99417",
   "metadata": {},
   "source": [
    "# Variable selection: an introduction\n",
    "\n",
    "Lino Galiana  \n",
    "2025-10-06\n",
    "\n",
    "<div class=\"badge-container\"><div class=\"badge-text\">If you want to try the examples in this tutorial:</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/en/modelisation/4_featureselection.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&name=«4_featureselection»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh»&init.personalInitArgs=«en/modelisation%204_featureselection%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=«4_featureselection»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh»&init.personalInitArgs=«en/modelisation%204_featureselection%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//en/blob/main//notebooks/en/modelisation/4_featureselection.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
    "\n",
    "Ce chapitre utilise toujours le même jeu de données, présenté dans l’[introduction\n",
    "de cette partie](index.qmd) : les données de vote aux élections présidentielles américaines\n",
    "croisées à des variables sociodémographiques.\n",
    "Le code\n",
    "est disponible [sur Github](https://github.com/linogaliana/python-datascientist/blob/main/content/modelisation/get_data.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d351392",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade xlrd #colab bug verson xlrd\n",
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0971c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/main/content/modelisation/get_data.py'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('getdata.py', 'wb').write(r.content)\n",
    "\n",
    "import getdata\n",
    "votes = getdata.create_votes_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b3a787-a989-4e15-adcf-f12f5d80d4ac",
   "metadata": {},
   "source": [
    "So far, we have assumed that the variables useful for predicting the Republican\n",
    "vote were known to the modeler. Thus, we have only used a limited portion of the\n",
    "available variables in our data. However, beyond the computational burden of building\n",
    "a model with a large number of variables, choosing a limited number of variables\n",
    "(a parsimonious model) reduces the risk of overfitting.\n",
    "\n",
    "How, then, can we choose the right number of variables and the best combination of these variables? There are multiple methods, including:\n",
    "\n",
    "-   Relying on statistical performance criteria that penalize non-parsimonious models. For example, BIC.\n",
    "-   *Backward elimination* techniques.\n",
    "-   Building models where the statistic of interest penalizes the lack of parsimony (which is what we aim to do here).\n",
    "\n",
    "In this chapter, we will present the main challenges of variable selection through LASSO.\n",
    "\n",
    "We will subsequently use the following functions or packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4406ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "import sklearn.metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import lasso_path\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e3814-1ee9-451b-930d-785aeecfefaf",
   "metadata": {},
   "source": [
    "# 1. The Principle of LASSO\n",
    "\n",
    "## 1.1 General Principle\n",
    "\n",
    "The class of *feature selection* models is very broad and includes\n",
    "a diverse range of models. We will focus on LASSO\n",
    "(*Least Absolute Shrinkage and Selection Operator*),\n",
    "which is an extension of linear regression aimed at selecting\n",
    "*sparse* models. This type of model is central to the field of\n",
    "*Compressed sensing* (where the term *L1-regularization* is more commonly used than LASSO). LASSO is a special case of\n",
    "elastic-net regressions, with another famous case being *ridge regression*.\n",
    "Unlike classical linear regression, these methods also work\n",
    "in a framework where $p>N$, i.e., where the number of predictors is much larger than\n",
    "the number of observations.\n",
    "\n",
    "## 1.2 Regularization\n",
    "\n",
    "By adopting the principle of a penalized objective function,\n",
    "LASSO allows certain coefficients to be set to 0.\n",
    "Variables with non-zero norms thus pass the selection test.\n",
    "\n",
    "> **Tip**\n",
    ">\n",
    "> LASSO is a constrained optimization problem. It seeks to find the estimator $\\beta$ that minimizes the quadratic error (linear regression) under an additional constraint regularizing the parameters:\n",
    "> $$\n",
    "> \\min_{\\beta} \\frac{1}{2}\\mathbb{E}\\bigg( \\big( X\\beta - y  \\big)^2 \\bigg) \\\\ \n",
    "> \\text{s.t. } \\sum_{j=1}^p |\\beta_j| \\leq t\n",
    "> $$\n",
    ">\n",
    "> This program is reformulated using the Lagrangian, allowing for a more tractable minimization program:\n",
    ">\n",
    "> $$\n",
    "> \\beta^{\\text{LASSO}} = \\arg \\min_{\\beta} \\frac{1}{2}\\mathbb{E}\\bigg( \\big( X\\beta - y  \\big)^2 \\bigg) + \\alpha \\sum_{j=1}^p |\\beta_j| = \\arg \\min_{\\beta} ||y-X\\beta||_{2}^{2} + \\lambda ||\\beta||_1\n",
    "> $$\n",
    ">\n",
    "> where $\\lambda$ is a reformulation of the previous regularization term, depending on $\\alpha$. The strength of the penalty applied to non-parsimonious models depends on this parameter.\n",
    "\n",
    "## 1.3 First LASSO Regression\n",
    "\n",
    "As we aim to find the\n",
    "best predictors of the Republican vote,\n",
    "we will remove variables\n",
    "that can be directly derived from these: the competitors’ scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef72bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df2 = pd.DataFrame(votes.drop(columns='geometry'))\n",
    "df2 = df2.loc[\n",
    "  :,\n",
    "  ~df2.columns.str.endswith(\n",
    "    ('_democrat','_green','_other', 'winner', 'per_point_diff', 'per_dem')\n",
    "    )\n",
    "  ]\n",
    "\n",
    "\n",
    "df2 = df2.loc[:,~df2.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7165853b-0f6c-4705-b1d8-4b137a5755a4",
   "metadata": {},
   "source": [
    "In this exercise, we will also use\n",
    "a function to extract\n",
    "the variables selected by LASSO,\n",
    "here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def extract_features_selected(lasso: Pipeline, preprocessing_step_name: str = 'preprocess') -> pd.Series:\n",
    "    \"\"\"\n",
    "    Extracts selected features based on the coefficients obtained from Lasso regression.\n",
    "\n",
    "    Parameters:\n",
    "    - lasso (Pipeline): The scikit-learn pipeline containing a trained Lasso regression model.\n",
    "    - preprocessing_step_name (str): The name of the preprocessing step in the pipeline. Default is 'preprocess'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: A Pandas Series containing selected features with non-zero coefficients.\n",
    "    \"\"\"\n",
    "    # Check if lasso object is provided\n",
    "    if not isinstance(lasso, Pipeline):\n",
    "        raise ValueError(\"The provided lasso object is not a scikit-learn pipeline.\")\n",
    "\n",
    "    # Extract the final transformer from the pipeline\n",
    "    lasso_model = lasso[-1]\n",
    "\n",
    "    # Check if lasso_model is a Lasso regression model\n",
    "    if not isinstance(lasso_model, Lasso):\n",
    "        raise ValueError(\"The final step of the pipeline is not a Lasso regression model.\")\n",
    "\n",
    "    # Check if lasso model has 'coef_' attribute\n",
    "    if not hasattr(lasso_model, 'coef_'):\n",
    "        raise ValueError(\"The provided Lasso regression model does not have 'coef_' attribute. \"\n",
    "                         \"Make sure it is a trained Lasso regression model.\")\n",
    "\n",
    "    # Get feature names from the preprocessing step\n",
    "    features_preprocessing = lasso[preprocessing_step_name].get_feature_names_out()\n",
    "\n",
    "    # Extract selected features based on non-zero coefficients\n",
    "    features_selec = pd.Series(features_preprocessing[np.abs(lasso_model.coef_) > 0])\n",
    "\n",
    "    return features_selec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbec8cd-3b08-4751-9ba8-0d22e6832cc2",
   "metadata": {},
   "source": [
    "> **Exercise 1: First LASSO**\n",
    ">\n",
    "> We are still trying to predict the variable `per_gop`. Before making our estimation, we will create certain intermediate objects to define our *pipeline*:\n",
    ">\n",
    "> 1.  In our `DataFrame`, replace infinite values with `NaN`.\n",
    ">\n",
    "> 2.  Create a training sample and a test sample.\n",
    ">\n",
    "> Now we can move on to defining our *pipeline*.\n",
    "> [This example](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) might serve as inspiration, as well as [this one](https://www.kaggle.com/code/bextuychiev/lasso-regression-with-pipelines-tutorial).\n",
    ">\n",
    "> 1.  First, create the *preprocessing* steps for our model.\n",
    ">     For this, it is common to separate the steps applied to continuous numerical variables from those applied to categorical variables.\n",
    ">\n",
    "> -   For **numerical variables**, impute with the mean and then standardize;\n",
    "> -   For **categorical variables**, linear regression techniques require using one-hot encoding. Before performing one-hot encoding, impute with the most frequent value.\n",
    ">\n",
    "> 1.  Finalize the *pipeline* by adding the estimation step and then estimate a LASSO model penalized with $\\alpha = 0.1$.\n",
    ">\n",
    "> Assuming your *pipeline* is stored in an object named `pipeline` and the last step is named `model`, you can directly access this step using the object `pipeline['model']`.\n",
    ">\n",
    "> 1.  Display the coefficient values. Which variables have a non-zero value?\n",
    "> 2.  Show that the selected variables are sometimes highly correlated.\n",
    "> 3.  Compare the performance of this parsimonious model with that of a model with more variables.\n",
    ">\n",
    "> <details>\n",
    ">\n",
    "> <summary>\n",
    ">\n",
    "> Help for Question 1\n",
    ">\n",
    "> </summary>\n",
    ">\n",
    "> ``` python\n",
    "> # Replace infinities with NaN\n",
    "> df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "> ```\n",
    ">\n",
    "> </details>\n",
    ">\n",
    "> <details>\n",
    ">\n",
    "> <summary>\n",
    ">\n",
    "> Help for Question 3\n",
    ">\n",
    "> </summary>\n",
    ">\n",
    "> The pipeline definition follows this structure:\n",
    ">\n",
    "> ``` python\n",
    "> numeric_pipeline = Pipeline(steps=[\n",
    ">     ('impute', # define the imputation method here\n",
    ">      ),\n",
    ">     ('scale', # define the standardization method here\n",
    ">     )\n",
    "> ])\n",
    ">\n",
    "> categorical_pipeline = # adapt the template\n",
    ">\n",
    "> # Define numerical_features and categorical_features beforehand\n",
    "> preprocessor = ColumnTransformer(transformers=[\n",
    ">     ('number', numeric_pipeline, numerical_features),\n",
    ">     ('category', categorical_pipeline, categorical_features)\n",
    "> ])\n",
    "> ```\n",
    ">\n",
    "> </details>\n",
    "\n",
    "The *preprocessing* pipeline (question 3) takes the following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99870bbf-f816-4e28-81f0-f901be29b0c8",
   "metadata": {},
   "source": [
    "The *pipeline* takes the following form once finalized (question 4):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dea2ee8-0bd6-41f8-9aca-62720eca7fdb",
   "metadata": {},
   "source": [
    "At the end of question 5, the selected variables are:\n",
    "\n",
    "The model is quite parsimonious as it uses a subset of our initial variables (especially since our categorical variables have been split into numerous variables through *one hot encoding*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78efc428-8832-4557-b94b-7f55e423fccc",
   "metadata": {},
   "source": [
    "Some variables make sense, such as education-related variables. Notably, one of the best predictors for the Republican score in 2020 is… the Republican score (and mechanically the Democratic score) in 2016 and 2012.\n",
    "\n",
    "Additionally, redundant variables are being selected. A more thorough data cleaning phase would actually be necessary.\n",
    "\n",
    "The parsimonious model is (slightly) more performant:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95336912-6ba6-4cdf-a733-6c933939f70b",
   "metadata": {},
   "source": [
    "Moreover, it can already be noted that regressing the 2020 score on the 2016 score results in very good explanatory performance, suggesting that voting behaves like an autoregressive process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257038cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "smf.ols(\"per_gop ~ share_2016_republican\", data = df2).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba6456-4722-4f7c-85da-9fdd6b2aec9b",
   "metadata": {},
   "source": [
    "# 2. Role of the Penalty $\\alpha$ in Variable Selection\n",
    "\n",
    "So far, we have taken the hyperparameter $\\alpha$\n",
    "as given. What role does it play in the conclusions of\n",
    "our modeling? To investigate this, we can explore the effect\n",
    "of its value on the number of variables passing the selection step.\n",
    "\n",
    "For the next exercise, we will consider exclusively\n",
    "quantitative variables to speed up the computations.\n",
    "Indeed, with non-parsimonious models, the multiple\n",
    "categories of our categorical variables make the optimization problem\n",
    "difficult to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8df133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/runner/work/python-datascientist/python-datascientist/.venv/lib/python3.12/site-packages/sklearn/impute/_base.py:635: UserWarning:\n",
      "\n",
      "Skipping features without any observed values: ['POV04_2021' 'CI90LB04_2021' 'CI90UB04_2021' 'PCTPOV04_2021'\n",
      " 'CI90LB04P_2021' 'CI90UB04P_2021']. At least one non-missing value is needed for imputation with strategy='mean'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df2.drop([\"per_gop\"], axis = 1),\n",
    "    100*df2[['per_gop']], test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "numerical_features = X_train.select_dtypes(include='number').columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(exclude='number').columns.tolist()\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "preprocessed_features = pd.DataFrame(\n",
    "  numeric_pipeline.fit_transform(\n",
    "    X_train.drop(columns = categorical_features)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbbb29-662e-4d31-a11a-0bd28d25bc3d",
   "metadata": {},
   "source": [
    "> **Exercise 2: Role of the Penalty Parameter**\n",
    ">\n",
    "> Use the `lasso_path` function to evaluate the number of parameters selected by LASSO as $\\alpha$\n",
    "> varies (explore $\\alpha \\in [0.001,0.01,0.02,0.025,0.05,0.1,0.25,0.5,0.8,1.0]$).\n",
    "\n",
    "The relationship you should obtain between $\\alpha$ and\n",
    "the number of parameters is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d2325-28cf-414a-b103-457b9c605923",
   "metadata": {},
   "source": [
    "We see that the higher $\\alpha$ is, the fewer variables the model selects.\n",
    "\n",
    "# 3. Cross-Validation to Select the Model\n",
    "\n",
    "Which $\\alpha$ should be preferred? For this,\n",
    "cross-validation should be performed to choose the model\n",
    "for which the variables passing the selection phase best predict\n",
    "the Republican outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "my_alphas = np.array([0.001,0.01,0.02,0.025,0.05,0.1,0.25,0.5,0.8,1.0])\n",
    "\n",
    "lcv = (\n",
    "  LassoCV(\n",
    "    alphas=my_alphas,\n",
    "    fit_intercept=False,\n",
    "    random_state=0,\n",
    "    cv=5\n",
    "    ).fit(\n",
    "      preprocessed_features, y_train\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3c9c9-0144-42dd-83aa-85447ed89cc1",
   "metadata": {},
   "source": [
    "The *“best”* $\\alpha$ can be retrieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a3018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha optimal : 1.0"
     ]
    }
   ],
   "source": [
    "print(\"alpha optimal :\", lcv.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d3ad1e-9f37-4303-864b-3cd98fc83f21",
   "metadata": {},
   "source": [
    "This can be used to run a new *pipeline*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a54aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('one-hot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('number', numeric_pipeline, numerical_features),\n",
    "    ('category', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "model = Lasso(\n",
    "  fit_intercept=False, \n",
    "  alpha = lcv.alpha_\n",
    ")  \n",
    "\n",
    "lasso_pipeline = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "lasso_optimal = lasso_pipeline.fit(X_train,y_train)\n",
    "\n",
    "features_selec2 = extract_features_selected(lasso_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a578acd-89ae-4206-8ba9-bddda20b68f7",
   "metadata": {},
   "source": [
    "Les variables sélectionnées sont :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc84a42-1789-4046-8771-8ba55a9d0fa7",
   "metadata": {},
   "source": [
    "Cela correspond à un modèle avec 13 variables sélectionnées.\n",
    "\n",
    "> **Tip**\n",
    ">\n",
    "> If the model appears to be insufficiently parsimonious, it would be necessary to revisit the variable definition phase to determine whether different scales for some variables might be more appropriate (e.g., using the `log`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/runner/work/python-datascientist/python-datascientist/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
